{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor\n",
    "from openpyxl import Workbook\n",
    "import pyLDAvis\n",
    "import warnings\n",
    "import os\n",
    "import pyLDAvis.sklearn\n",
    "import pandas as pd\n",
    "import jieba\n",
    "warnings.filterwarnings('ignore')\n",
    "df1 = pd.read_csv(\"data/weibo.csv\", encoding='utf-8')\n",
    "df=pd.DataFrame(df1['text'].astype(str))\n",
    "\n",
    "#加载停用词表\n",
    "a=r'停用词表stopwords\\746.txt'\n",
    "b=r'停用词表stopwords\\902.txt'\n",
    "c=r'停用词表stopwords\\1208.txt'\n",
    "d=r'停用词表stopwords\\1447.txt'\n",
    "e=r'停用词表stopwords\\1893.txt'\n",
    "f=r'停用词表stopwords\\中文.txt'\n",
    "g=r'停用词表stopwords\\哈工大停用词表.txt'\n",
    "h=r'停用词表stopwords\\四川大学机器智能实验室停用词库.txt'\n",
    "i=r'停用词表stopwords\\百度停用词列表.txt'\n",
    "j=r'停用词表stopwords\\符号.txt'\n",
    "k=r'停用词表stopwords\\英文.txt'\n",
    "l=r'停用词表stopwords\\stopwords.txt'\n",
    "lis=[a,b,c,d,e,f,g,h,i,j,k,l]\n",
    "if os.path.exists('data/单个文本主题.xlsx'):\n",
    "    os.remove('data/单个文本主题.xlsx')\n",
    "\n",
    "def loadstop_words(path):\n",
    "    # 停用词：结合多个停用词表\n",
    "    stop_words = set()\n",
    "    for path in lis:\n",
    "        with open(path,'r',encoding='utf8') as fr:\n",
    "            for line in fr:\n",
    "                item = line.strip()\n",
    "                stop_words.add(item)\n",
    "    custom_stop_words=set()\n",
    "    stop_words=stop_words | custom_stop_words\n",
    "    return stop_words\n",
    "stopwords=loadstop_words(lis)\n",
    "\n",
    "#分词，并添加限制条件分词\n",
    "def chinese_word_cut(text):\n",
    "    s=list(jieba.cut(text))\n",
    "    s1=[]\n",
    "    for word in s:\n",
    "        if len(word) <= 2 and word not in stopwords and not word.isdigit():\n",
    "            s1.append(word)\n",
    "    if not s1:\n",
    "        return '空'\n",
    "    else:\n",
    "        return ' '.join(s1)  \n",
    "df['text_cutted']=df.text.apply(chinese_word_cut)\n",
    "# print(df.text_cutted.loc[[2]])\n",
    "# print(df.iloc[[1]])\n",
    "lens=len(df['text_cutted'])\n",
    "n_features = 1000    \n",
    "n_topics = 1\n",
    "n_top_words = 5\n",
    "\n",
    "def save_topic(model, feature_names, n_top_words ,topid):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append(['topic','words'])\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"第%d条短文本主题:\" % (topid+1))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        line=[topid+1 ,\" \".join([feature_names[i]for i in topic.argsort()[:-n_top_words - 1:-1]])]\n",
    "        ws.append(line)  # 将数据需要保存的项以行的形式添加到xlsx中\n",
    "        wb.save(r'D:\\pycharm\\爬虫\\data\\topic.xlsx')        \n",
    "    print()\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = 5,\n",
    "                                min_df = 1)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "def oneto():\n",
    "    i=0\n",
    "    while i<lens:\n",
    "        try:\n",
    "            df1=df['text_cutted'].iloc[[i]]   \n",
    "            tf= tf_vectorizer.fit_transform(df1)\n",
    "            tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "            lda.fit(tf)\n",
    "            save_topic(lda, tf_feature_names, n_top_words,i)\n",
    "            i=i+1       \n",
    "        except ValueError as e:\n",
    "    #         print(\"第%d条短文本为特殊文本无主题(如仅包含停用词):\" % (i+1))\n",
    "            i=i+1\n",
    "            continue\n",
    "    return\n",
    "# executor=ProcessPoolExecutor(100)\n",
    "# executor.submit(oneto)\n",
    "oneto()\n",
    "#整体提取\n",
    "n_features =len(df.text_cutted)\n",
    "n_topics = 20\n",
    "n_top_words1 = 15\n",
    "tf1_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 2)\n",
    "# tf1_feature_names = tf1_vectorizer.get_feature_names()\n",
    "tf1 = tf1_vectorizer.fit_transform(df.text_cutted)\n",
    "lda1 = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda1.fit(tf1)\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "# print_top_words(lda, tf_feature_names, n_top_words1)\n",
    "\n",
    "data = pyLDAvis.sklearn.prepare(lda1, tf1, tf1_vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
