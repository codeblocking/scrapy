{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条短文本主题:\n",
      "预售 岁月 喜欢 回头 本书\n",
      "\n",
      "第2条短文本主题:\n",
      "机会 上次 路线 钢琴 艺术\n",
      "\n",
      "第3条短文本主题:\n",
      "出门 主子 感受 日常 影子\n",
      "\n",
      "第4条短文本主题:\n",
      "真好\n",
      "\n",
      "第6条短文本主题:\n",
      "姿势 一架 记得 记仇 呼噜\n",
      "\n",
      "第8条短文本主题:\n",
      "最右\n",
      "\n",
      "第9条短文本主题:\n",
      "烧烤\n",
      "\n",
      "第10条短文本主题:\n",
      "十一 不信 仓鼠 颜值 小麦\n",
      "\n",
      "第11条短文本主题:\n",
      "转眼 收到 粽子\n",
      "\n",
      "第12条短文本主题:\n",
      "劝醒 姐妹\n",
      "\n",
      "第13条短文本主题:\n",
      "领域 擅长 邂逅\n",
      "\n",
      "第14条短文本主题:\n",
      "计划 失败 恋爱\n",
      "\n",
      "第15条短文本主题:\n",
      "钢琴 朗朗 复课 妻子 羡慕\n",
      "\n",
      "第16条短文本主题:\n",
      "抖森 同框 天猫\n",
      "\n",
      "第17条短文本主题:\n",
      "赶紧 快乐 接收\n",
      "\n",
      "第18条短文本主题:\n",
      "办法 不见 耗在 说服 永远\n",
      "\n",
      "第19条短文本主题:\n",
      "丰盛 速来\n",
      "\n",
      "第20条短文本主题:\n",
      "狗狗 路上\n",
      "\n",
      "第21条短文本主题:\n",
      "恭喜\n",
      "\n",
      "第22条短文本主题:\n",
      "一条 超全\n",
      "\n",
      "第23条短文本主题:\n",
      "老师 技术 整容 这位\n",
      "\n",
      "第24条短文本主题:\n",
      "防晒 晒黑 卖得 平价 跟风\n",
      "\n",
      "第25条短文本主题:\n",
      "钢丝 护理 姐妹 安利 粗糙\n",
      "\n",
      "第26条短文本主题:\n",
      "北京 去世 世园 好吃 请问\n",
      "\n",
      "第27条短文本主题:\n",
      "帖子 夏天 小吃 问问 续命\n",
      "\n",
      "第28条短文本主题:\n",
      "心动 公交 形状 看过 玉玺\n",
      "\n",
      "第29条短文本主题:\n",
      "套上 出门 外卖 睡裙 安利\n",
      "\n",
      "第30条短文本主题:\n",
      "北极 兔界 逗笑 动图 瞬间\n",
      "\n",
      "第31条短文本主题:\n",
      "有心\n",
      "\n",
      "第32条短文本主题:\n",
      "记得 还记\n",
      "\n",
      "第33条短文本主题:\n",
      "平复 久久 妹子 心情 微信\n",
      "\n",
      "第36条短文本主题:\n",
      "傻纸\n",
      "\n",
      "第37条短文本主题:\n",
      "海豹\n",
      "\n",
      "第40条短文本主题:\n",
      "超级\n",
      "\n",
      "第43条短文本主题:\n",
      "知识\n",
      "\n",
      "第44条短文本主题:\n",
      "相爱 始终 孤独 陪伴\n",
      "\n",
      "第45条短文本主题:\n",
      "倒闭 款上\n",
      "\n",
      "第47条短文本主题:\n",
      "真香\n",
      "\n",
      "第49条短文本主题:\n",
      "调包 公司 厉害\n",
      "\n",
      "第51条短文本主题:\n",
      "奖励 两箱 配合 今日 罐头\n",
      "\n",
      "第52条短文本主题:\n",
      "假期\n",
      "\n",
      "第55条短文本主题:\n",
      "喜欢 嘻嘻 一首 拍摄 阿初\n",
      "\n",
      "第56条短文本主题:\n",
      "夕阳 无限\n",
      "\n",
      "第57条短文本主题:\n",
      "大风 太阳\n",
      "\n",
      "第58条短文本主题:\n",
      "吹风 太阳\n",
      "\n",
      "第60条短文本主题:\n",
      "钥匙 室友 不到 出门 昨天\n",
      "\n",
      "第61条短文本主题:\n",
      "想法 一件 私密 音乐 歌单\n",
      "\n",
      "第63条短文本主题:\n",
      "草莓 天使 次郎 野田\n",
      "\n",
      "第64条短文本主题:\n",
      "剧情 钢铁 漫威 终意 唯粉\n",
      "\n",
      "第65条短文本主题:\n",
      "弗兰 米粉\n",
      "\n",
      "第66条短文本主题:\n",
      "还行 眉毛 眼睛\n",
      "\n",
      "第67条短文本主题:\n",
      "宝宝 善良 姨母 生菜 感动\n",
      "\n",
      "第68条短文本主题:\n",
      "这期 韩网\n",
      "\n",
      "第69条短文本主题:\n",
      "一句\n",
      "\n",
      "第71条短文本主题:\n",
      "追星 姐妹 脑洞 酒店\n",
      "\n",
      "第75条短文本主题:\n",
      "球球\n",
      "\n",
      "第78条短文本主题:\n",
      "欺骗 世界 充满\n",
      "\n",
      "第79条短文本主题:\n",
      "作业 哥哥\n",
      "\n",
      "第80条短文本主题:\n",
      "女人 为难\n",
      "\n",
      "第81条短文本主题:\n",
      "实力\n",
      "\n",
      "第82条短文本主题:\n",
      "丰盛 午餐\n",
      "\n",
      "第83条短文本主题:\n",
      "蝴蝶 仙子 快乐\n",
      "\n",
      "第84条短文本主题:\n",
      "赛前 亲密 礼仪\n",
      "\n",
      "第85条短文本主题:\n",
      "书包\n",
      "\n",
      "第86条短文本主题:\n",
      "夏天 中国 躁动 血液 传统\n",
      "\n",
      "第87条短文本主题:\n",
      "恐龙 科普 课程 唯一 提供\n",
      "\n",
      "第88条短文本主题:\n",
      "烘焙 好吃 妈妈 辛苦\n",
      "\n",
      "第89条短文本主题:\n",
      "回复\n",
      "\n",
      "第90条短文本主题:\n",
      "恭喜\n",
      "\n",
      "第91条短文本主题:\n",
      "高高\n",
      "\n",
      "第92条短文本主题:\n",
      "铜表 挂绿 身上\n",
      "\n",
      "第94条短文本主题:\n",
      "大风 人生 吹得 怀疑\n",
      "\n",
      "第95条短文本主题:\n",
      "包子\n",
      "\n",
      "第96条短文本主题:\n",
      "不吃 包子\n",
      "\n",
      "第98条短文本主题:\n",
      "早安\n",
      "\n",
      "第99条短文本主题:\n",
      "晚安\n",
      "\n",
      "第100条短文本主题:\n",
      "晚安\n",
      "\n",
      "第101条短文本主题:\n",
      "早安\n",
      "\n",
      "第102条短文本主题:\n",
      "晚安\n",
      "\n",
      "第103条短文本主题:\n",
      "有奖\n",
      "\n",
      "第104条短文本主题:\n",
      "早安\n",
      "\n",
      "第105条短文本主题:\n",
      "晚安\n",
      "\n",
      "第106条短文本主题:\n",
      "儿童 礼物\n",
      "\n",
      "第107条短文本主题:\n",
      "送个 树洞 置顶\n",
      "\n",
      "第108条短文本主题:\n",
      "闲鱼\n",
      "\n",
      "第109条短文本主题:\n",
      "姐妹\n",
      "\n",
      "第110条短文本主题:\n",
      "总让 不贵 东西 舒服 男友\n",
      "\n",
      "第112条短文本主题:\n",
      "算是\n",
      "\n",
      "第113条短文本主题:\n",
      "集锦 一组 发言\n",
      "\n",
      "第114条短文本主题:\n",
      "聊聊\n",
      "\n",
      "第115条短文本主题:\n",
      "无瓜 雨女\n",
      "\n",
      "第116条短文本主题:\n",
      "自卑 女孩 投稿\n",
      "\n",
      "第119条短文本主题:\n",
      "平时 产品 群体 驱蚊 算是\n",
      "\n",
      "第121条短文本主题:\n",
      "开船 无证\n",
      "\n",
      "第122条短文本主题:\n",
      "平台\n",
      "\n",
      "第123条短文本主题:\n",
      "姑娘 长成\n",
      "\n",
      "第124条短文本主题:\n",
      "取乐 食用\n",
      "\n",
      "第125条短文本主题:\n",
      "玉米 回来 水果\n",
      "\n",
      "第126条短文本主题:\n",
      "话题 一盏 明灯\n",
      "\n",
      "第127条短文本主题:\n",
      "右边\n",
      "\n",
      "第128条短文本主题:\n",
      "简单 幸福 床上 舒服\n",
      "\n",
      "第129条短文本主题:\n",
      "才华\n",
      "\n",
      "第130条短文本主题:\n",
      "一年 两斤\n",
      "\n",
      "第131条短文本主题:\n",
      "科学 工资 感觉\n",
      "\n",
      "第133条短文本主题:\n",
      "一买\n",
      "\n",
      "第134条短文本主题:\n",
      "权力 七季 人生 烂尾 游戏\n",
      "\n",
      "第135条短文本主题:\n",
      "表白 挫折 消化\n",
      "\n",
      "第136条短文本主题:\n",
      "扔掉\n",
      "\n",
      "第140条短文本主题:\n",
      "带上 创作 季来 狂欢\n",
      "\n",
      "第143条短文本主题:\n",
      "话题 互动 干货\n",
      "\n",
      "第146条短文本主题:\n",
      "话题 完剧 尽兴\n",
      "\n",
      "第147条短文本主题:\n",
      "速码 策略 话题\n",
      "\n",
      "第150条短文本主题:\n",
      "恭喜 锦鲤\n",
      "\n",
      "第151条短文本主题:\n",
      "凌晨 一年 错过 欧冠 决战\n",
      "\n",
      "第152条短文本主题:\n",
      "欧洲 之巅 决战\n",
      "\n",
      "第153条短文本主题:\n",
      "欧冠 压轴 大戏 足球\n",
      "\n",
      "第155条短文本主题:\n",
      "恭喜\n",
      "\n",
      "第156条短文本主题:\n",
      "客场 一轮 法甲 完美 收官\n",
      "\n",
      "第157条短文本主题:\n",
      "半价 兑换 公仔 限时\n",
      "\n",
      "第158条短文本主题:\n",
      "我怒 不知 半天 老婆 生气\n",
      "\n",
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Jun/2019 21:23:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2019 21:23:17] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2019 21:23:17] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jun/2019 21:23:17] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor\n",
    "from openpyxl import Workbook\n",
    "import pyLDAvis\n",
    "import warnings\n",
    "import os\n",
    "import pyLDAvis.sklearn\n",
    "import pandas as pd\n",
    "import jieba\n",
    "warnings.filterwarnings('ignore')\n",
    "df1 = pd.read_csv(\"data/weibo.csv\", encoding='utf-8')\n",
    "df=pd.DataFrame(df1['text'].astype(str))\n",
    "\n",
    "#加载停用词表\n",
    "a=r'停用词表stopwords\\746.txt'\n",
    "b=r'停用词表stopwords\\902.txt'\n",
    "c=r'停用词表stopwords\\1208.txt'\n",
    "d=r'停用词表stopwords\\1447.txt'\n",
    "e=r'停用词表stopwords\\1893.txt'\n",
    "f=r'停用词表stopwords\\中文.txt'\n",
    "g=r'停用词表stopwords\\哈工大停用词表.txt'\n",
    "h=r'停用词表stopwords\\四川大学机器智能实验室停用词库.txt'\n",
    "i=r'停用词表stopwords\\百度停用词列表.txt'\n",
    "j=r'停用词表stopwords\\符号.txt'\n",
    "k=r'停用词表stopwords\\英文.txt'\n",
    "l=r'停用词表stopwords\\stopwords.txt'\n",
    "lis=[a,b,c,d,e,f,g,h,i,j,k,l]\n",
    "if os.path.exists('data/单个文本主题.xlsx'):\n",
    "    os.remove('data/单个文本主题.xlsx')\n",
    "\n",
    "def loadstop_words(path):\n",
    "    # 停用词：结合多个停用词表\n",
    "    stop_words = set()\n",
    "    for path in lis:\n",
    "        with open(path,'r',encoding='utf8') as fr:\n",
    "            for line in fr:\n",
    "                item = line.strip()\n",
    "                stop_words.add(item)\n",
    "    custom_stop_words=set()\n",
    "    stop_words=stop_words | custom_stop_words\n",
    "    return stop_words\n",
    "stopwords=loadstop_words(lis)\n",
    "\n",
    "#分词，并添加限制条件分词\n",
    "def chinese_word_cut(text):\n",
    "    s=list(jieba.cut(text))\n",
    "    s1=[]\n",
    "    for word in s:\n",
    "        if len(word) <= 2 and word not in stopwords and not word.isdigit():\n",
    "            s1.append(word)\n",
    "    if not s1:\n",
    "        return '空'\n",
    "    else:\n",
    "        return ' '.join(s1)  \n",
    "df['text_cutted']=df.text.apply(chinese_word_cut)\n",
    "# print(df.text_cutted)\n",
    "# print(df.iloc[[1]])\n",
    "lens=len(df['text_cutted'])\n",
    "n_features = 1000    \n",
    "n_topics = 1\n",
    "n_top_words = 5\n",
    "\n",
    "def save_topic(model, feature_names, n_top_words ,topid):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append(['topic','words'])\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"第%d条短文本主题:\" % (topid+1))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        line=[topid+1 ,\" \".join([feature_names[i]for i in topic.argsort()[:-n_top_words - 1:-1]])]\n",
    "        ws.append(line)  # 将数据需要保存的项以行的形式添加到xlsx中\n",
    "        wb.save(r'D:\\pycharm\\爬虫\\data\\topic.xlsx')        \n",
    "    print()\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = 5,\n",
    "                                min_df = 1)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "def oneto():\n",
    "    i=0\n",
    "    while i<lens:\n",
    "        try:\n",
    "            df1=df['text_cutted'].iloc[[i]]   \n",
    "            tf= tf_vectorizer.fit_transform(df1)\n",
    "            tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "            lda.fit(tf)\n",
    "            save_topic(lda, tf_feature_names, n_top_words,i)\n",
    "            i=i+1       \n",
    "        except ValueError as e:\n",
    "    #         print(\"第%d条短文本为特殊文本无主题(如仅包含停用词):\" % (i+1))\n",
    "            i=i+1\n",
    "            continue\n",
    "    return\n",
    "# executor=ProcessPoolExecutor(100)\n",
    "# executor.submit(oneto)\n",
    "oneto()\n",
    "#整体提取\n",
    "n_features =len(df.text_cutted)\n",
    "n_topics = 20\n",
    "n_top_words1 = 15\n",
    "tf1_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords,\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 2)\n",
    "# tf1_feature_names = tf1_vectorizer.get_feature_names()\n",
    "tf1 = tf1_vectorizer.fit_transform(df.text_cutted)\n",
    "lda1 = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda1.fit(tf1)\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "# print_top_words(lda, tf_feature_names, n_top_words1)\n",
    "\n",
    "data = pyLDAvis.sklearn.prepare(lda1, tf1, tf1_vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
